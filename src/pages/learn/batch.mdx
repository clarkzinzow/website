---
title: Delta Operations
width: full
---

import { ApacheSpark, DeltaLake } from "/src/components/Attributions";
import { SparkSession } from "/src/components/DeltaLakeConcepts";
import { CBCreateTable } from "/src/components/CodeBlocks";
import "react-tabs/style/react-tabs.css";
import * as React from "react";

## Table Operations

<DeltaLake /> supports most of the options provided by <ApacheSpark /> DataFrame
read and write APIs for performing batch reads and writes on tables. For
many <DeltaLake /> operations on tables, you enable integration
with <ApacheSpark /> DataSourceV2 and Catalog APIs (since 3.0) by setting
configurations when you create a new <SparkSession />.

<br />

For many <DeltaLake /> operations on tables, you enable integration with

<ApacheSpark /> DataSourceV2 and Catalog APIs (since 3.0) by setting
configurations when you create a new <SparkSession />.

## Create a Table

You can create tables in the following ways:

- SQL DDL commands: You can use standard SQL DDL commands supported in Apache Spark (for example, CREATE TABLE and REPLACE TABLE) to create Delta tables:

<CBCreateTable
    sql={
    <div>

        ```sql
        CREATE TABLE IF NOT EXISTS default.people10m (
        id INT,
        firstName STRING,
        middleName STRING,
        lastName STRING,
        gender STRING,
        birthDate TIMESTAMP,
        ssn STRING,
        salary INT
        ) USING DELTA

        CREATE OR REPLACE TABLE default.people10m (
        id INT,
        firstName STRING,
        middleName STRING,
        lastName STRING,
        gender STRING,
        birthDate TIMESTAMP,
        ssn STRING,
        salary INT
        ) USING DELTA
        ```
    </div>

}
/>

SQL also supports creating a table at a path, without creating an entry in the Hive metastore.

<CBCreateTable sql={
    <div>

    ```sql
    -- Create or replace table with path
    CREATE OR REPLACE TABLE delta.`/tmp/delta/people10m` (
    id INT,
    firstName STRING,
    middleName STRING,
    lastName STRING,
    gender STRING,
    birthDate TIMESTAMP,
    ssn STRING,
    salary INT
    ) USING DELTA
    ```

    </div>
    }

/>

- DataFrameWriter API: If you want to simultaneously create a table and insert data into it from Spark DataFrames or Datasets, you can use the Spark DataFrameWriter (Scala or Java and Python).

<CBCreateTable python={
    <div>

    ```python
    # Create table in the metastore using DataFrame's schema and write data to it
    df.write.format("delta").saveAsTable("default.people10m")

    # Create or replace partitioned table with path using DataFrame's schema and write/overwrite data to it
    df.write.format("delta").mode("overwrite").save("/tmp/delta/people10m")
    ```

    </div>
    }
    scala={
    <div>

    ```scala
    // Create table in the metastore using DataFrame's schema and write data to it
    df.write.format("delta").saveAsTable("default.people10m")

    // Create table with path using DataFrame's schema and write data to it
    df.write.format("delta").mode("overwrite").save("/tmp/delta/people10m")
    ```

    </div>
    }

/>

You can also create Delta tables using the Spark DataFrameWriterV2 API.

- DeltaTableBuilder API: You can also use the DeltaTableBuilder API in Delta Lake to create tables. Compared to the DataFrameWriter APIs, this API makes it easier to specify additional information like column comments, table properties, and generated columns.

  <Info title="Note" level="info">
    This feature is new and is in Preview.
  </Info>

  <CBCreateTable python={
  <div>

        ```python
        # Create table in the metastore
        DeltaTable.createIfNotExists(spark) \
        .tableName("default.people10m") \
        .addColumn("id", "INT") \
        .addColumn("firstName", "STRING") \
        .addColumn("middleName", "STRING") \
        .addColumn("lastName", "STRING", comment = "surname") \
        .addColumn("gender", "STRING") \
        .addColumn("birthDate", "TIMESTAMP") \
        .addColumn("ssn", "STRING") \
        .addColumn("salary", "INT") \
        .execute()

        # Create or replace table with path and add properties
        DeltaTable.createOrReplace(spark) \
        .addColumn("id", "INT") \
        .addColumn("firstName", "STRING") \
        .addColumn("middleName", "STRING") \
        .addColumn("lastName", "STRING", comment = "surname") \
        .addColumn("gender", "STRING") \
        .addColumn("birthDate", "TIMESTAMP") \
        .addColumn("ssn", "STRING") \
        .addColumn("salary", "INT") \
        .property("description", "table with people data") \
        .location("/tmp/delta/people10m") \
        .execute()
        ```

    </div>
  }
  scala={
    <div>
      ```scala
      // Create table in the metastore
      DeltaTable.createOrReplace(spark)
      .tableName("default.people10m")
      .addColumn("id", "INT")
      .addColumn("firstName", "STRING")
      .addColumn("middleName", "STRING")
      .addColumn(
      DeltaTable.columnBuilder("lastName")
      .dataType("STRING")
      .comment("surname")
      .build())
      .addColumn("lastName", "STRING", comment = "surname")
      .addColumn("gender", "STRING")
      .addColumn("birthDate", "TIMESTAMP")
      .addColumn("ssn", "STRING")
      .addColumn("salary", "INT")
      .execute()

        // Create or replace table with path and add properties
        DeltaTable.createOrReplace(spark)
        .addColumn("id", "INT")
        .addColumn("firstName", "STRING")
        .addColumn("middleName", "STRING")
        .addColumn(
        DeltaTable.columnBuilder("lastName")
        .dataType("STRING")
        .comment("surname")
        .build())
        .addColumn("lastName", "STRING", comment = "surname")
        .addColumn("gender", "STRING")
        .addColumn("birthDate", "TIMESTAMP")
        .addColumn("ssn", "STRING")
        .addColumn("salary", "INT")
        .property("description", "table with people data")
        .location("/tmp/delta/people10m")
        .execute()
        ```

    </div>
  }
/>
